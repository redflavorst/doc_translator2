# services/translation_service.py
from pathlib import Path
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import time
import threading
import requests
import re
import json

try:
    from bs4 import BeautifulSoup  # HTML í‘œ íŒŒì‹±ìš©
    BS4_AVAILABLE = True
except Exception:
    BS4_AVAILABLE = False

# ê¸°ì¡´ ë²ˆì—­ê¸° ì„í¬íŠ¸
#from quality_contract_translator import QualityContractTranslator

@dataclass
class TranslationResult:
    """ë²ˆì—­ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    success: bool
    input_file: str
    output_file: str
    report: Dict[str, Any]
    processing_time: float
    error: Optional[str] = None

@dataclass
class BatchTranslationResult:
    """ë°°ì¹˜ ë²ˆì—­ ê²°ê³¼"""
    success: bool
    results: List[TranslationResult]
    total_files: int
    successful_files: int
    failed_files: int
    average_confidence: float
    total_processing_time: float
    low_quality_files: List[str]

class TranslationService:
    """quality_contract_translator.pyì˜ QualityContractTranslatorë¥¼ ì„œë¹„ìŠ¤ë¡œ ë˜í•‘"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        

        # ì²­í¬ ì„¤ì • ì¶”ê°€ (ë¹ ì§„ ë¶€ë¶„)
        self.CHUNK_SIZE = 500  # ê¸°ë³¸ ì²­í¬ í¬ê¸° - ì´ ì¤„ ì¶”ê°€

        # ë²ˆì—­ê¸° ì„¤ì •
        self.model_name = self.config.get('model_name', 'qwen3:8b')
        self.temperature = self.config.get('temperature', 0.1)
        self.max_retries = self.config.get('max_retries', 3)
        self.ollama_url = self.config.get('ollama_url', 'http://localhost:11434')
        self.quality_threshold = self.config.get('quality_threshold', 0.6)
        
        # í‘œ ë²ˆì—­ ë°°ì¹˜ ì„¤ì • (quality_contract_translator ê¸°ì¤€ ê°’ ì°¨ìš©)
        self.SHORT_CELL_MAX_CHARS = 50
        self.SHORT_CELL_BATCH_SIZE = 120
        self.MEDIUM_CELL_MAX_CHARS = 200  # ìƒˆë¡œ ì¶”ê°€
        self.LONG_CELL_MAX_CHARS = 1000  # ìƒˆë¡œ ì¶”ê°€
        self.MAX_CHARS_PER_BATCH = 2000
        
        # ë°°ì¹˜ í¬ê¸° ë™ì  ì„¤ì •
        self.CELL_BATCH_CONFIGS = {
            'short': {'batch_size': 20, 'num_predict': 500},
            'medium': {'batch_size': 10, 'num_predict': 1000},
            'long': {'batch_size': 3, 'num_predict': 2000},
            'extra_long': {'batch_size': 1, 'num_predict': 4000}
        }


        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ (ì¤‘ë³µ ì…€ ë²ˆì—­ ìµœì†Œí™”)
        self._cell_cache: Dict[str, str] = {}
        
        # ìŠ¤ë ˆë“œë³„ HTTP ì„¸ì…˜ë§Œ ë¶„ë¦¬ (ë²ˆì—­ê¸°ëŠ” ê³µìœ  ì„¤ì • ì‚¬ìš©)
        self._local = threading.local()

        #ì „ì—­ ë²ˆì—­ê¸° ìƒì„± ì¹´ìš´í„° (ë””ë²„ê¹…ìš©?)
        self._session_count = 0
        self._lock = threading.Lock()

        #Ollama ì„œë²„ ìƒíƒœ í™•ì¸
        self._check_ollama_server()

    def _classify_cell(self, text: str) -> str:
        """ì…€ í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë¶„ë¥˜"""
        length = len(text.strip())
        
        if length <= self.SHORT_CELL_MAX_CHARS:
            return 'short'
        elif length <= self.MEDIUM_CELL_MAX_CHARS:
            return 'medium'
        elif length <= self.LONG_CELL_MAX_CHARS:
            return 'long'
        else:
            return 'extra_long'

    def _check_ollama_server(self):
        """ Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/version",timeout=5)
            if response.status_code == 200:
                print(f"âœ… Ollama ì„œë²„ ì—°ê²° í™•ì¸: {self.ollama_url}")
                print(f"ğŸ“Š ê³µìœ  ëª¨ë¸: {self.model_name}")
            else:
                print(f"âš ï¸ Ollama ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
        except Exception as e:
            print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print(f"   í™•ì¸ì‚¬í•­: ollama serve ì‹¤í–‰ ì—¬ë¶€")
    
    def _get_session(self) -> requests.Session:
        """ìŠ¤ë ˆë“œë³„ HTTP ì„¸ì…˜ íšë“"""
        
        # í˜„ì¬ ìŠ¤ë ˆë“œì— ì„¸ì…˜ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒì„±
        if not hasattr(self._local, 'session') or self._local.session is None:
            
            # ìŠ¤ë ˆë“œ ì •ë³´
            thread_id = threading.current_thread().ident
            thread_name = threading.current_thread().name
            
            with self._lock:
                self._session_count += 1
                current_count = self._session_count
            
            print(f"\n{'='*50}")
            print(f"ğŸŒ ìŠ¤ë ˆë“œë³„ HTTP ì„¸ì…˜ ìƒì„±")
            print(f"   ìŠ¤ë ˆë“œ ID: {thread_id}")
            print(f"   ìŠ¤ë ˆë“œ ì´ë¦„: {thread_name}")
            print(f"   ì„¸ì…˜ ìˆœì„œ: {current_count}ë²ˆì§¸")
            print(f"   Ollama ì„œë²„: {self.ollama_url}")
            print(f"   ê³µìœ  ëª¨ë¸: {self.model_name}")
            print(f"{'='*50}")
            
            # ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ HTTP ì„¸ì…˜ ìƒì„±
            self._local.session = requests.Session()
            
            # ì„¸ì…˜ ì„¤ì • ìµœì í™”
            self._local.session.timeout = 300
            self._local.session.headers.update({
                'Content-Type': 'application/json',
                'User-Agent': f'DocumentTranslator-Thread-{thread_id}'
            })
            
            print(f"âœ… ìŠ¤ë ˆë“œ {thread_id}: HTTP ì„¸ì…˜ ìƒì„± ì™„ë£Œ!")
            print(f"   ì„¸ì…˜ ID: {id(self._local.session)}")
            print(f"   íƒ€ì„ì•„ì›ƒ: 300ì´ˆ")
            print(f"{'='*50}\n")
                
        return self._local.session

    # 5. translate_document ë©”ì„œë“œ ìˆ˜ì • (ë¡œê¹… ì¶”ê°€)
    def translate_document(self, input_file: str, output_file: str = None) -> TranslationResult:
        """ë‹¨ì¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ë²ˆì—­ (ìŠ¤ë ˆë“œ ì•ˆì „)
            Args:
                input_file: ë²ˆì—­í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
                output_file: ë²ˆì—­ ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
                
            Returns:
                TranslationResult: ë²ˆì—­ ê²°ê³¼
        
        """
        thread_id = threading.current_thread().ident
        start_time = time.time()
        print(f"\nğŸ“ ìŠ¤ë ˆë“œ {thread_id}: ë²ˆì—­ ì‹œì‘ - {input_file}")
        
        try:
            # ì…ë ¥ íŒŒì¼ ê²€ì¦
            input_path = Path(input_file)
            if not input_path.exists():
                raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            if output_file is None:
                output_file = str(input_path.parent / f"{input_path.stem}_korean{input_path.suffix}")
            
            self.logger.info(f"ìŠ¤ë ˆë“œ {thread_id}: ë¬¸ì„œ ë²ˆì—­ ì‹œì‘ - {input_path.name}")

            # ìŠ¤ë ˆë“œë³„ ë…ë¦½ HTTP ì„¸ì…˜ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ì‹¤í–‰
            session = self._get_session()
            report = self._translate_with_session(session,str(input_file), output_file)

            processing_time = time.time() - start_time
            
            # ë²ˆì—­ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            success = (
                report.get('average_confidence', 0) >= self.quality_threshold and
                Path(output_file).exists()
            )

            confidence = report.get('average_confidence', 0)
            print(f"âœ… ìŠ¤ë ˆë“œ {thread_id}: ë²ˆì—­ ì™„ë£Œ!")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   ì‹ ë¢°ë„: {confidence:.3f}")
            print(f"   ì„±ê³µ ì—¬ë¶€: {success}")
            
            self.logger.info(f"ë¬¸ì„œ ë²ˆì—­ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {report.get('average_confidence', 0):.2f}")
            
            return TranslationResult(
                success=success,
                input_file=str(input_file),
                output_file=output_file,
                report=report,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ ìŠ¤ë ˆë“œ {thread_id}: ë²ˆì—­ ì‹¤íŒ¨ - {e}")
            self.logger.error(f"ìŠ¤ë ˆë“œ {thread_id}: ë¬¸ì„œ ë²ˆì—­ ì‹¤íŒ¨: {e}")

            return TranslationResult(
                success=False,
                input_file=str(input_file),
                output_file=output_file or "",
                report={},
                processing_time=time.time() - start_time,
                error=str(e)
            )
        
    def _translate_with_session(self, session: requests.Session, input_file: str, output_file: str) -> dict:
        """íŠ¹ì • HTTP ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ì‹¤í–‰"""
        thread_id = threading.current_thread().ident
        
        try:
            # íŒŒì¼ ì½ê¸°
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í‘œ í¬í•¨ ì‹œ ì „ìš© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
            used_table_pipeline = False
            if ('<table' in content.lower()) and BS4_AVAILABLE:
                print(f"ğŸ§© ìŠ¤ë ˆë“œ {thread_id}: HTML í‘œ ê°ì§€ â†’ í‘œ ì „ìš© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
                final_translation, table_stats = self._translate_markdown_with_tables(session, content)
                sections_count = table_stats.get('sections_count', 0)
                avg_confidence = table_stats.get('avg_confidence', 1.0)
                used_table_pipeline = True
            else:
                # ì²­í¬ë¡œ ë¶„í• 
                chunks = self._split_into_chunks(content)
                translated_chunks = []
                
                print(f"ğŸ“„ ìŠ¤ë ˆë“œ {thread_id}: {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
                
                total_confidence = 0.0
                
                for i, chunk in enumerate(chunks, 1):
                    print(f"ğŸ”„ ìŠ¤ë ˆë“œ {thread_id}: ì²­í¬ {i}/{len(chunks)} ë²ˆì—­ ì¤‘...")
                    
                    # Ollama API í˜¸ì¶œ (ë…ë¦½ ì„¸ì…˜ ì‚¬ìš©)
                    translated_text, confidence = self._translate_chunk_ollama(
                        session, chunk, i, len(chunks)
                    )
                    
                    translated_chunks.append(translated_text)
                    total_confidence += confidence
                    
                    # ì ì‹œ ëŒ€ê¸° (Ollama ì„œë²„ ë¶€í•˜ ë¶„ì‚°)
                    time.sleep(0.1)
                
                # ë²ˆì—­ ê²°ê³¼ í•©ì¹˜ê¸°
                final_translation = '\n\n'.join(translated_chunks)
                sections_count = len(chunks)
                avg_confidence = total_confidence / len(chunks) if chunks else 0
            
            # íŒŒì¼ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_translation)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            return {
                'input_file': input_file,
                'output_file': output_file,
                'sections_count': sections_count,
                'average_confidence': avg_confidence,
                'total_processing_time': time.time(),
                'model_used': self.model_name,
                'thread_id': thread_id,
                'session_id': id(session),
                'used_table_pipeline': used_table_pipeline
            }
            
        except Exception as e:
            print(f"âŒ ìŠ¤ë ˆë“œ {thread_id}: ë²ˆì—­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
        
    def _translate_chunk_ollama(self, session: requests.Session, text: str, 
                               idx: int, total: int) -> Tuple[str, float]:
        """Ollamaë¥¼ ì‚¬ìš©í•œ ì²­í¬ ë²ˆì—­ (ë…ë¦½ ì„¸ì…˜ ì‚¬ìš©)"""
        
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë²•ë¬´/ê¸°ìˆ  ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

ê·œì¹™:
- êµ¬ì¡°/ì„œì‹ ë³´ì¡´(ë§ˆí¬ë‹¤ìš´/HTML ìœ ì§€)
- í•´ì„¤/ì¶”ë¡ /ë¨¸ë¦¬ë§ ê¸ˆì§€
- ì¶œë ¥ í˜•ì‹ ê°•ì œ: ìµœì¢… ë²ˆì—­ë¬¸ë§Œ <md>ì™€ </md> ì‚¬ì´ì— ë„£ê³ , ê·¸ ì™¸ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€

ì›ë¬¸:
{text}

ì¶œë ¥:"""
        
        data = {
            'model': self.model_name,
            'prompt': prompt,
            'stream': False,
            'options': {
                'temperature': self.temperature,
                'num_predict': 4000,
                'stop': ['ì›ë¬¸:', 'Translation:', 'ë²ˆì—­:', '\nì›ë¬¸'],
            }
        }
        
        for attempt in range(self.max_retries):
            try:
                # ìŠ¤ë ˆë“œë³„ ë…ë¦½ ì„¸ì…˜ ì‚¬ìš©
                response = session.post(
                    f'{self.ollama_url}/api/generate',
                    json=data,
                    timeout=300
                )
                response.raise_for_status()
                
                result = response.json()
                translated_text = result.get('response', '').strip()
                
                # ë²ˆì—­ ê²°ê³¼ í›„ì²˜ë¦¬: <md> ë¸”ë¡ ì¶”ì¶œ ë° ë©”íƒ€ ì œê±° â†’ ê°„ë‹¨ ì •ë¦¬
                translated_text = self._strip_meta(translated_text)
                translated_text = self._clean_translation(translated_text)
                
                # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_confidence(text, translated_text)
                
                return translated_text, confidence
                
            except Exception as e:
                thread_id = threading.current_thread().ident
                print(f"âš ï¸ ìŠ¤ë ˆë“œ {thread_id}: Ollama ìš”ì²­ ì‹¤íŒ¨ (ì²­í¬ {idx}, ì‹œë„ {attempt+1}): {e}")
                if attempt == self.max_retries - 1:
                    return f"[ë²ˆì—­ ì‹¤íŒ¨: {str(e)}]", 0.0
                time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    def _clean_translation(self, text: str) -> str:
        """ë²ˆì—­ ê²°ê³¼ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        text = re.sub(r'^ë²ˆì—­:\s*', '', text)
        text = re.sub(r'^Translation:\s*', '', text)
        text = re.sub(r'\në²ˆì—­:\s*', '\n', text)
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

    # -------------------- í‘œ/ë§ˆí¬ë‹¤ìš´ ì „ìš© ì²˜ë¦¬ --------------------
    def _translate_markdown_with_tables(self, session: requests.Session, content: str) -> Tuple[str, Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ ë‚´ HTML í‘œ ë¸”ë¡ì„ ì…€ ë‹¨ìœ„/í–‰ ë‹¨ìœ„ ë°°ì¹˜ ë²ˆì—­ í›„ ì¬ì¡°ë¦½.
        í‘œ ì™¸ í…ìŠ¤íŠ¸ëŠ” ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë²ˆì—­.
        """
        start = time.time()
        parts: List[str] = []
        total_sections = 0
        confidences: List[float] = []

        # 1ì°¨: <div> ë˜í•‘ëœ <table> ìš°ì„  ì²˜ë¦¬
        pattern = re.compile(r"(<div[\s\S]*?<table[\s\S]*?</table>[\s\S]*?</div>)", re.IGNORECASE)
        cursor = 0
        found_any = False
        for m in pattern.finditer(content):
            found_any = True
            # í‘œ ë¸”ë¡ ì „ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            before = content[cursor:m.start()]
            if before.strip():
                segments = self._split_into_chunks(before, max_chunk_size=1200)
                for seg in segments:
                    res_text, conf = self._translate_chunk_ollama(session, seg, 1, 1)
                    parts.append(res_text)
                    confidences.append(conf)
                    total_sections += 1

            table_block = m.group(1)
            translated_block = self._translate_html_table_block(session, table_block)
            parts.append(translated_block)
            cursor = m.end()

        if not found_any:
            # 2ì°¨: ìˆœìˆ˜ <table> ë¸”ë¡ ì²˜ë¦¬ (wrapper ì—†ìŒ)
            pattern2 = re.compile(r"(<table[\s\S]*?</table>)", re.IGNORECASE)
            cursor = 0
            for m in pattern2.finditer(content):
                found_any = True
                before = content[cursor:m.start()]
                if before.strip():
                    segments = self._split_into_chunks(before, max_chunk_size=1200)
                    for seg in segments:
                        res_text, conf = self._translate_chunk_ollama(session, seg, 1, 1)
                        parts.append(res_text)
                        confidences.append(conf)
                        total_sections += 1
                table_block = m.group(1)
                translated_block = self._translate_html_table_block(session, table_block)
                parts.append(translated_block)
                cursor = m.end()
            tail = content[cursor:]
            if tail.strip():
                segments = self._split_into_chunks(tail, max_chunk_size=1200)
                for seg in segments:
                    res_text, conf = self._translate_chunk_ollama(session, seg, 1, 1)
                    parts.append(res_text)
                    confidences.append(conf)
                    total_sections += 1
        else:
            # ë§ˆì§€ë§‰ ê¼¬ë¦¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            tail = content[cursor:]
            if tail.strip():
                segments = self._split_into_chunks(tail, max_chunk_size=1200)
                for seg in segments:
                    res_text, conf = self._translate_chunk_ollama(session, seg, 1, 1)
                    parts.append(res_text)
                    confidences.append(conf)
                    total_sections += 1

        elapsed = time.time() - start
        avg_conf = sum(confidences)/len(confidences) if confidences else 1.0
        return ('\n'.join(parts), {"sections_count": total_sections, "avg_confidence": avg_conf, "total_time": elapsed})

    def _translate_html_table_block(self, session: requests.Session, html_block: str) -> str:
        """HTML í‘œ ë¸”ë¡ì—ì„œ ì…€ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ë²ˆì—­í•˜ê³  ì¹˜í™˜.
        ì§§ì€ ì…€ì€ ì…€ ë‹¨ìœ„, ê¸´ ì…€ì€ JSON ë°°ì¹˜ ë²ˆì—­.
        """
        if not BS4_AVAILABLE:
            return html_block

        soup = BeautifulSoup(html_block, 'html.parser')
        tables = soup.find_all('table')
        for table in tables:
            try:
                self._normalize_html_table(table)
            except Exception:
                pass
            rows = table.find_all('tr')
            # í—¤ë” ì…€ ìš°ì„  ì²˜ë¦¬
            header_cells = table.find_all(['th'])
            if not header_cells:
                # <th>ê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ì²˜ëŸ¼ ì²˜ë¦¬
                first_tr = table.find('tr')
                if first_tr:
                    header_cells = first_tr.find_all('td')
            self._translate_cells_list(session, header_cells, force_cell=True)

            # ë³¸ë¬¸ ì…€ ì²˜ë¦¬
            body_rows: List[List] = []
            for tr in rows:
                cells = tr.find_all(['td'])
                if cells:
                    body_rows.append(cells)
            self._translate_table_rows(session, body_rows)

        return str(soup)

    def _normalize_html_table(self, table) -> None:
        """í‘œ êµ¬ì¡° ì •ê·œí™”: ë¹ˆ í–‰ ì œê±°, ì´ì–´ì“°ê¸° ë³‘í•©, ì—´ ê°œìˆ˜ íŒ¨ë”©."""
        def cells_in_row(tr):
            return tr.find_all(['td', 'th'])

        rows = table.find_all('tr')
        max_cols = 0
        for tr in rows:
            max_cols = max(max_cols, len(cells_in_row(tr)))

        prev_tr = None
        for tr in list(rows):
            cells = cells_in_row(tr)
            texts = [c.get_text(" ", strip=True) for c in cells]
            non_empty_idx = [i for i, t in enumerate(texts) if t]

            # 1) ì™„ì „ ë¹ˆ í–‰ ì œê±°
            if not non_empty_idx:
                tr.decompose()
                continue

            # 2) ì´ì–´ì“°ê¸° ë³‘í•©(íœ´ë¦¬ìŠ¤í‹±)
            if len(non_empty_idx) == 1 and prev_tr is not None:
                only_idx = non_empty_idx[0]
                t = texts[only_idx]
                t_low = t.lower()
                is_continuation = (
                    t.startswith((')', '-', 'Â·', 'â€¢')) or
                    t.endswith(')') or
                    any(tok in t_low for tok in ['signer', 'joint', 'continued', 'cont.', 'and'])
                )
                if is_continuation:
                    prev_cells = cells_in_row(prev_tr)
                    prev_texts = [pc.get_text(" ", strip=True) for pc in prev_cells]
                    if any(prev_texts):
                        last_idx = max(i for i, s in enumerate(prev_texts) if s)
                        prev_cells[last_idx].string = (prev_texts[last_idx] + ' ' + t).strip()
                        tr.decompose()
                        continue

            # 3) ì—´ ê°œìˆ˜ íŒ¨ë”©
            if len(cells) < max_cols:
                for _ in range(max_cols - len(cells)):
                    new_td = table.new_tag('td')
                    tr.append(new_td)

            prev_tr = tr

    def _translate_table_rows(self, session: requests.Session, rows: List[List]) -> None:
        """ì…€ ê¸¸ì´ì— ë”°ë¥¸ ì ì‘í˜• ë²ˆì—­"""
    
        # 1. ëª¨ë“  ì…€ì„ ê¸¸ì´ë³„ë¡œ ë¶„ë¥˜
        cell_buckets = {
            'short': [],
            'medium': [],
            'long': [],
            'extra_long': []
        }

        for row_idx, row in enumerate(rows):
            for col_idx, cell in enumerate(row):
                text = (cell.get_text(" ", strip=True) or "").strip()
                if not text:
                    continue
                
                cell_type = self._classify_cell(text)
                cell_buckets[cell_type].append({
                    'cell': cell,
                    'text': text,
                    'row': row_idx,
                    'col': col_idx
                })

        # 2. ê° ë²„í‚·ë³„ë¡œ ìµœì í™”ëœ ì²˜ë¦¬
        for cell_type, cells_info in cell_buckets.items():
            if not cells_info:
                continue
            
            config = self.CELL_BATCH_CONFIGS[cell_type]
            
            if cell_type in ['short', 'medium']:
                self._batch_translate_cells(session, cells_info, config)
            elif cell_type == 'long':
                self._translate_long_cells(session, cells_info, config)
            else:  # extra_long
                self._translate_extra_long_cells(session, cells_info, config)

    
    def _translate_cells_list(self, session: requests.Session, cells: List, force_cell: bool) -> None:
        """ì…€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ë²ˆì—­(JSON) + ê°„ë‹¨ ìºì‹œ."""
        items: List[Dict[str, str]] = []
        originals: List[str] = []
        for idx, cell in enumerate(cells):
            text = (cell.get_text(" ", strip=True) or "").strip()
            if not text:
                continue
            if not force_cell and len(text) > self.SHORT_CELL_MAX_CHARS:
                continue  # ê¸´ ì…€ì€ í–‰ ë‹¨ìœ„ ì²˜ë¦¬
            key = self._cache_key(text)
            if key in self._cell_cache:
                cell.string = self._cell_cache[key]
            else:
                items.append({"id": f"c{idx}", "text": text})
                originals.append(text)

        # ë°°ì¹˜ ì²˜ë¦¬
        pending: List[Dict[str, str]] = []
        acc_len = 0
        def flush_batch(batch: List[Dict[str, str]]):
            if not batch:
                return
            results = self._translate_batch_json(session, batch, fast=True)
            for obj in batch:
                cid = obj["id"]
                original = obj["text"]
                translated = self._strip_meta(results.get(cid) or original) if results else original
                key = self._cache_key(original)
                self._cell_cache[key] = translated

        for obj in items:
            cur_len = len(obj["text"]) + 40
            if (len(pending) >= self.SHORT_CELL_BATCH_SIZE) or (acc_len + cur_len > self.MAX_CHARS_PER_BATCH):
                flush_batch(pending)
                pending = []
                acc_len = 0
            pending.append(obj)
            acc_len += cur_len
        flush_batch(pending)

        # ì¹˜í™˜
        for cell in cells:
            text = (cell.get_text(" ", strip=True) or "").strip()
            if not text:
                continue
            if not force_cell and len(text) > self.SHORT_CELL_MAX_CHARS:
                continue
            key = self._cache_key(text)
            if key in self._cell_cache:
                cell.string = self._cell_cache[key]


    def _batch_translate_cells(self, session: requests.Session, cells_info: List[Dict], config: Dict) -> None:
        """ì§§ì€/ì¤‘ê°„ ì…€ ë°°ì¹˜ ì²˜ë¦¬"""
        batch_size = config['batch_size']

        for i in range(0, len(cells_info), batch_size):
            batch = cells_info[i:i+batch_size]

            #ìºì‹œ í™•ì¸ ë° í•„í„°ë§
            to_translate = []
            for info in batch:
                cache_key = self._cache_key(info['text'])
                if cache_key not in self._cell_cache:
                    to_translate.append(info)
            if not to_translate:
                continue

            # JSON ë°°ì¹˜ ìƒì„±
            items = [{"id":f"c{j}", "text":info['text']} for j, info in enumerate(to_translate)]

            #ë™ì  í† í° ê³„ì‚°
            total_chars = sum(len(info['text']) for info in to_translate)
            num_predict = min(total_chars*2, config['num_predict'])

            #ë²ˆì—­ ì‹¤í–‰
            results = self._translate_batch_json_adaptive(session, items, num_predict=num_predict)

            # ê²°ê³¼ ì ìš©
            for info, item in zip(to_translate, items):
                translated = results.get(item['id'], info['text'])
                cache_key = self._cache_key(info['text'])
                self._cell_cache[cache_key] = translated
                info['cell'].string = translated

    def _translate_long_cells(self, session: requests.Session, 
                         cells_info: List[Dict], config: Dict) -> None:
        """ê¸´ ì…€ ê°œë³„ ì²˜ë¦¬"""
        for info in cells_info:
            text = info['text']
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ê°ì§€
            if '\n' in text and text.count('\n') > 3:
                translated = self._translate_as_list(session, text, config)
            else:
                # ì¼ë°˜ ë¬¸ë‹¨ìœ¼ë¡œ ì²˜ë¦¬
                translated, _ = self._translate_chunk_ollama(
                    session, text, 1, 1
                )
            
            info['cell'].string = translated
            time.sleep(0.1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    def _translate_extra_long_cells(self, session: requests.Session,
                               cells_info: List[Dict], config: Dict) -> None:
        """ë§¤ìš° ê¸´ ì…€ ì²­í¬ ì²˜ë¦¬"""
        for info in cells_info:
            text = info['text']
            chunks = self._smart_chunk(text, chunk_size=self.CHUNK_SIZE)
            
            translated_chunks = []
            for i, chunk in enumerate(chunks):
                # ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
                context = ""
                if i > 0 and translated_chunks:
                    # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ
                    last_chunk = translated_chunks[-1]
                    sentences = last_chunk.split('.')
                    if len(sentences) > 1:
                        context = sentences[-2] + '.' if sentences[-2] else ""
                
                prompt = f"""
                {f'ì´ì „ ë¬¸ë§¥: {context}' if context else ''}
                
                ë‹¤ìŒ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”:
                {chunk}
                
                ë²ˆì—­:
                """
                
                translated, _ = self._translate_chunk_ollama(
                    session, prompt, i+1, len(chunks)
                )
                translated_chunks.append(translated)
            
            info['cell'].string = ' '.join(translated_chunks)

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ì¶”ê°€
    def _smart_chunk(self, text: str, chunk_size: int = 500) -> List[str]:
        """ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ë³´ì¡´í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ì²­í‚¹"""
        chunks = []
        
        # ì¤„ë°”ê¿ˆì´ ë§ìœ¼ë©´ ì¤„ ë‹¨ìœ„ë¡œ ì²­í‚¹
        if text.count('\n') > 5:
            lines = text.split('\n')
            current_chunk = []
            current_size = 0
            
            for line in lines:
                line_size = len(line)
                if current_size + line_size > chunk_size and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_size = line_size
                else:
                    current_chunk.append(line)
                    current_size += line_size
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
        
        else:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹
            sentences = re.split(r'(?<=[.!?])\s+', text)
            current_chunk = []
            current_size = 0
            
            for sentence in sentences:
                sent_size = len(sentence)
                if current_size + sent_size > chunk_size and current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = [sentence]
                    current_size = sent_size
                else:
                    current_chunk.append(sentence)
                    current_size += sent_size
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
        
        return chunks if chunks else [text]    

    def _translate_as_list(self, session: requests.Session, 
                       text: str, config: Dict) -> str:
        """ë¦¬ìŠ¤íŠ¸ í˜•íƒœ í…ìŠ¤íŠ¸ ìµœì í™” ë²ˆì—­"""
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # 5-7ê°œì”© ê·¸ë£¹í™”
        groups = []
        for i in range(0, len(lines), 5):
            groups.append(lines[i:i+5])
        
        translated_groups = []
        for group in groups:
            group_text = '\n'.join(group)
            prompt = f"""
            ë‹¤ìŒ í•­ëª©ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš” (í•œ ì¤„ì— í•˜ë‚˜ì”©):
            
            {group_text}
            
            ë²ˆì—­:
            """
            
            translated, _ = self._translate_chunk_ollama(
                session, prompt, 1, 1
            )
            translated_groups.append(translated)
        
        return '\n'.join(translated_groups)
    
    def _translate_batch_json_adaptive(self, session: requests.Session, 
                                  items: List[Dict], num_predict: int) -> Dict[str, str]:
        """ì ì‘í˜• JSON ë°°ì¹˜ ë²ˆì—­"""
        if not items:
            return {}
        
        prompt = f"""
        ë‹¤ìŒ í•­ëª©ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
        JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”: [{{"id": "...", "translated": "..."}}]
        
        ì…ë ¥:
        {json.dumps(items, ensure_ascii=False, indent=2)}
        
        ì¶œë ¥:
        """
        
        # Ollama í˜¸ì¶œ with ë™ì  ì„¤ì •
        data = {
            'model': self.model_name,
            'prompt': prompt,
            'stream': False,
            'format': 'json',
            'options': {
                'temperature': 0.1,
                'num_predict': num_predict,
                'num_ctx': 4096  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™•ëŒ€
            }
        }
        
        try:
            response = session.post(
                f'{self.ollama_url}/api/generate',
                json=data,
                timeout=300
            )
            
            if response.status_code == 200:
                result_text = response.json().get('response', '')
                # JSON íŒŒì‹±
                result_data = json.loads(result_text)
                return {item['id']: item.get('translated', '') 
                    for item in result_data}
        except Exception as e:
            print(f"ë°°ì¹˜ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            
        return {}

    # -------------------- LLM í˜¸ì¶œ ìœ í‹¸ --------------------
    def _translate_batch_json(self, session: requests.Session, items: List[Dict[str, str]], fast: bool = True) -> Dict[str, str]:
        if not items:
            return {}
        instruction = (
            "You are a translator. Translate each item's 'text' from English to Korean.\n"
            "Respond ONLY with a compact JSON array: [{\"id\": string, \"translated\": string}, ...].\n"
            "Do not add any explanation."
        )
        examples = json.dumps(items, ensure_ascii=False)
        prompt = f"{instruction}\n\nINPUT:\n{examples}\n\nOUTPUT:"
        resp = self._generate_once_fast(session, prompt) if fast else self._generate_once(session, prompt)
        try:
            m = re.search(r'\[.*\]', resp or '', flags=re.DOTALL)
            payload = m.group(0) if m else (resp or '[]')
            data = json.loads(payload)
            out = {d.get('id'): d.get('translated', '') for d in data if isinstance(d, dict)}
            return out
        except Exception:
            # í´ë°±: í•œ ê°œì”© ì²˜ë¦¬
            out: Dict[str, str] = {}
            for obj in items:
                text = obj["text"]
                single = self._generate_once(
                    session,
                    "Translate to Korean. Return ONLY the translated text, no explanations:\n" +
                    f"{text}\n\nOUTPUT:"
                ) or text
                out[obj["id"]] = self._strip_meta(single)
            return out

    def _generate_once_fast(self, session: requests.Session, prompt: str) -> Optional[str]:
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "format": "json",
            "stream": False,
            "options": {
                "temperature": 0.0,
                "top_p": 1.0,
                "num_predict": 256
            },
        }
        try:
            response = session.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60,
            )
            if response.status_code == 200:
                raw = response.json().get("response", "").strip()
                return raw
        except Exception:
            pass
        return None

    def _generate_once(self, session: requests.Session, prompt: str) -> Optional[str]:
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.temperature,
                "num_predict": 1024
            },
        }
        try:
            response = session.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=120,
            )
            if response.status_code == 200:
                return response.json().get("response", "").strip()
        except Exception:
            pass
        return None

    # -------------------- ë©”íƒ€/ìºì‹œ ìœ í‹¸ --------------------
    def _strip_meta(self, text: str) -> str:
        if not text:
            return text
        # think ë¸”ë¡ ì œê±°
        t = re.sub(r'(?is)<\s*think\s*>[\s\S]*?<\s*/\s*think\s*>', '', text)
        t = re.sub(r'(?is)</\s*think\s*>', '', t)
        # md ë¸”ë¡ì´ ìˆìœ¼ë©´ ë‚´ë¶€ë§Œ ì‚¬ìš©
        m = re.search(r'(?is)<\s*md\s*>([\s\S]*?)<\s*/\s*md\s*>', t)
        if m:
            t = m.group(1)
        # ì•ìª½ ì˜ì–´ í•´ì„¤ ì œê±°: ì²« í•œê¸€/í—¤ë”/HTMLë¶€í„°
        try:
            lines = t.splitlines()
            start = 0
            for i, line in enumerate(lines):
                if re.search(r'[ê°€-í£]', line) or line.strip().startswith(('#', '<')):
                    start = i
                    break
            t = "\n".join(lines[start:])
        except Exception:
            pass
        return t.strip()

    def _cache_key(self, text: str) -> str:
        norm = re.sub(r"\s+", " ", text.strip())
        try:
            import hashlib
            return hashlib.sha1(norm.encode("utf-8")).hexdigest()
        except Exception:
            return norm
    
    def _calculate_confidence(self, original: str, translated: str) -> float:
        """ë²ˆì—­ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not translated or translated.startswith('[ë²ˆì—­ ì‹¤íŒ¨'):
            return 0.0
        
        # ê¸¸ì´ ê¸°ë°˜ ì‹ ë¢°ë„
        length_ratio = len(translated) / max(len(original), 1)
        if length_ratio < 0.3 or length_ratio > 3.0:
            return 0.3
        
        # í•œêµ­ì–´ ë¬¸ì ë¹„ìœ¨
        korean_chars = sum(1 for char in translated if 'ê°€' <= char <= 'í£')
        korean_ratio = korean_chars / max(len(translated), 1)
        
        # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(0.9, 0.7 + korean_ratio * 0.2)
        return confidence
    
    def _split_into_chunks(self, content: str, max_chunk_size: int = 2000) -> list[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += ("\n\n" if current_chunk else "") + paragraph
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [content]

    # 6. ìƒˆë¡œ ì¶”ê°€í•  ë””ë²„ê¹… ë©”ì„œë“œ
    def get_thread_info(self) -> Dict[str, Any]:
        """ í˜„ì¬ ìŠ¤ë ˆë“œì˜ ë²ˆì—­ê¸° ì •ë³´ ë°˜í™˜(ë””ë²„ê¹…ìš©)"""
        thread_id = threading.current_thread().ident
        thread_name = threading.current_thread().name
        has_session = hasattr(self._local, 'session') and self._local.session is not None

        return {
            "thread_id": thread_id,
            "thread_name": thread_name,
            "has_session": has_session,
            "total_sessions_created":self._session_count,
            "session_id":id(self._local.session) if has_session else None,
            "ollama_model": self.model_name,
            "ollama_url": self.ollama_url
        } 
    def translate_documents_batch(self, markdown_files: List[str], output_dir: str = None) -> BatchTranslationResult:
        """
        ì—¬ëŸ¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°°ì¹˜ ë²ˆì—­
        
        Args:
            markdown_files: ë²ˆì—­í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            output_dir: ë²ˆì—­ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê° íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬)
            
        Returns:
            BatchTranslationResult: ë°°ì¹˜ ë²ˆì—­ ê²°ê³¼
        """
        start_time = time.time()
        results = []
        successful_files = 0
        failed_files = 0
        total_confidence = 0.0
        confidence_count = 0
        low_quality_files = []
        
        self.logger.info(f"ë°°ì¹˜ ë²ˆì—­ ì‹œì‘: {len(markdown_files)}ê°œ íŒŒì¼")
        
        for i, md_file in enumerate(markdown_files, 1):
            self.logger.info(f"ë²ˆì—­ ì§„í–‰ ì¤‘ ({i}/{len(markdown_files)}): {Path(md_file).name}")
            
            try:
                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì •
                input_path = Path(md_file)
                if output_dir:
                    output_path = Path(output_dir) / f"{input_path.stem}_korean{input_path.suffix}"
                    output_path.parent.mkdir(parents=True, exist_ok=True)
                    output_file = str(output_path)
                else:
                    output_file = None
                
                # ê°œë³„ íŒŒì¼ ë²ˆì—­
                result = self.translate_document(md_file, output_file)
                results.append(result)
                
                if result.success:
                    successful_files += 1
                    confidence = result.report.get('average_confidence', 0)
                    total_confidence += confidence
                    confidence_count += 1
                    
                    # í’ˆì§ˆì´ ë‚®ì€ íŒŒì¼ ê¸°ë¡
                    if confidence < self.quality_threshold:
                        low_quality_files.append(md_file)
                else:
                    failed_files += 1
                    
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë²ˆì—­ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({md_file}): {e}")
                failed_files += 1
                results.append(TranslationResult(
                    success=False,
                    input_file=md_file,
                    output_file="",
                    report={},
                    processing_time=0,
                    error=str(e)
                ))
        
        # ì „ì²´ ê²°ê³¼ ê³„ì‚°
        total_processing_time = time.time() - start_time
        average_confidence = total_confidence / confidence_count if confidence_count > 0 else 0.0
        
        batch_success = (failed_files == 0) and (len(low_quality_files) == 0)
        
        self.logger.info(f"ë°°ì¹˜ ë²ˆì—­ ì™„ë£Œ: {successful_files}/{len(markdown_files)} ì„±ê³µ, "
                        f"í‰ê·  ì‹ ë¢°ë„: {average_confidence:.2f}, "
                        f"ì´ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
        
        return BatchTranslationResult(
            success=batch_success,
            results=results,
            total_files=len(markdown_files),
            successful_files=successful_files,
            failed_files=failed_files,
            average_confidence=average_confidence,
            total_processing_time=total_processing_time,
            low_quality_files=low_quality_files
        )
    
    def retry_failed_translations(self, batch_result: BatchTranslationResult) -> BatchTranslationResult:
        """
        ì‹¤íŒ¨í•œ ë²ˆì—­ ì¬ì‹œë„
        
        Args:
            batch_result: ì´ì „ ë°°ì¹˜ ë²ˆì—­ ê²°ê³¼
            
        Returns:
            BatchTranslationResult: ì¬ì‹œë„ ê²°ê³¼
        """
        failed_files = [r.input_file for r in batch_result.results if not r.success]
        
        if not failed_files:
            self.logger.info("ì¬ì‹œë„í•  ì‹¤íŒ¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return batch_result
        
        self.logger.info(f"ì‹¤íŒ¨í•œ ë²ˆì—­ ì¬ì‹œë„: {len(failed_files)}ê°œ íŒŒì¼")
        
        # ì¬ì‹œë„ ì‹œ ì„¤ì • ì¡°ì • (ì˜¨ë„ ë‚®ì¶”ê¸°, ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€)
        original_config = self.config.copy()
        self.config['temperature'] = max(0.05, self.config.get('temperature', 0.1) - 0.05)
        self.config['max_retries'] = self.config.get('max_retries', 3) + 2
        
        # ë²ˆì—­ê¸° ì¬ì´ˆê¸°í™”
        self._translator = None
        
        try:
            retry_result = self.translate_documents_batch(failed_files)
            
            # ì„±ê³µí•œ ì¬ì‹œë„ ê²°ê³¼ë¡œ ì›ë³¸ ê²°ê³¼ ì—…ë°ì´íŠ¸
            updated_results = []
            retry_dict = {r.input_file: r for r in retry_result.results}
            
            for original_result in batch_result.results:
                if original_result.input_file in retry_dict:
                    retry_res = retry_dict[original_result.input_file]
                    if retry_res.success:
                        updated_results.append(retry_res)
                    else:
                        updated_results.append(original_result)  # ì—¬ì „íˆ ì‹¤íŒ¨
                else:
                    updated_results.append(original_result)  # ì›ë˜ ì„±ê³µí–ˆë˜ ê²ƒ
            
            # í†µê³„ ì¬ê³„ì‚°
            successful = sum(1 for r in updated_results if r.success)
            failed = len(updated_results) - successful
            
            avg_conf = sum(r.report.get('average_confidence', 0) 
                          for r in updated_results if r.success) / max(successful, 1)
            
            low_quality = [r.input_file for r in updated_results 
                          if r.success and r.report.get('average_confidence', 0) < self.quality_threshold]
            
            return BatchTranslationResult(
                success=(failed == 0 and len(low_quality) == 0),
                results=updated_results,
                total_files=len(updated_results),
                successful_files=successful,
                failed_files=failed,
                average_confidence=avg_conf,
                total_processing_time=batch_result.total_processing_time + retry_result.total_processing_time,
                low_quality_files=low_quality
            )
            
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self.config = original_config
            
    
    def get_translation_status(self, input_files: List[str], output_dir: str = None) -> Dict[str, Dict[str, Any]]:
        """
        ë²ˆì—­ ìƒíƒœ í™•ì¸
        
        Args:
            input_files: í™•ì¸í•  ì…ë ¥ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: íŒŒì¼ë³„ ë²ˆì—­ ìƒíƒœ
        """
        status = {}
        
        for input_file in input_files:
            input_path = Path(input_file)
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì¶”ì •
            if output_dir:
                output_path = Path(output_dir) / f"{input_path.stem}_korean{input_path.suffix}"
            else:
                output_path = input_path.parent / f"{input_path.stem}_korean{input_path.suffix}"
            
            file_status = {
                'input_exists': input_path.exists(),
                'output_exists': output_path.exists(),
                'input_size': input_path.stat().st_size if input_path.exists() else 0,
                'output_size': output_path.stat().st_size if output_path.exists() else 0,
                'needs_translation': not output_path.exists() or output_path.stat().st_mtime < input_path.stat().st_mtime
            }
            
            status[str(input_file)] = file_status
        
        return status
    
    def cleanup_temp_files(self, output_dir: str):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬ - ë””ìŠ¤í¬ì˜ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ"""
        try:
            output_path = Path(output_dir)
            if output_path.exists():
                # raw íŒŒì¼ë“¤ ì •ë¦¬
                for raw_file in output_path.glob("*_raw.md"):
                    raw_file.unlink()
                    self.logger.debug(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {raw_file}")
                
                # ìºì‹œ íŒŒì¼ ì •ë¦¬
                cache_file = output_path / "translation_cache.json"
                if cache_file.exists():
                    cache_file.unlink()
                    self.logger.debug(f"ìºì‹œ íŒŒì¼ ì‚­ì œ: {cache_file}")
                
                self.logger.info(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {output_dir}")
        except Exception as e:
            self.logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if hasattr(self._local, 'session') and self._local.session:
            self._local.session.close()
            self._local.session = None
            self.logger.info("HTTP ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
    
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.cleanup()
        except:
            pass